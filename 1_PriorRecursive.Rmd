---
title: "Prior Recursive"
author: "Alexander Berliner"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## (i) Simulating Data from a Logistic Regression Model with Synthetic Predictors

Logistic regression is a binary classifier. We want to find the probability that the observation $P(y = 1 \mid \mathbf{x})$ is a member of a class.

Along with the training set $\mathbf{X}$, we have a vector $\boldsymbol{\beta}$ of weights for each feature, including the intercept. The resulting evidence for the class is given by:

$$ z = \mathbf{X} \boldsymbol{\beta} $$

To convert $z$ into a probability, we pass it through the sigmoid (logistic) function:

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

**References**:

1.  <https://web.stanford.edu/~jurafsky/slp3/5.pdf>

**Setup:**

```{r}
#To be set by user:
n <- 2000 #number of observations (set it higher for consistency)
beta <- c(1, 2, 3)#, 4, 5, 6)  # True coefficients (bias, and predictors) px1

#Synthetic predictors
p <- length(beta)-1 #number of predictors
M <- p+1 #number of predictors plus intercept
library(MASS)
set.seed(7)
X <- cbind(1, matrix(mvrnorm(n, mu = rep(0, p), Sigma = 1*diag(p)), n, p))  # Design matrix with intercept (bias). nxp

#Response
z <- X %*% beta  # Linear predictor. %*% = matrix mult. nx1 dim
sig <- 1/(1+exp(-z)) #sigmoid (inv-logit) function, find probability in [0,1], predicted y
y <- rbinom(n=n, size=1, prob=sig) #size=num trials=1 -> bernoulli response var
```

## (iii) Implementing a recursive approach in which we consider batches of data in a sequence

**2.1) Prior-recursive Bayesian Inference**

Define functions:

```{r}
#Proposal distro
proposal <- function(mu, lambda, sigma) {
  # Draw a single sample from multivariate normal
  distro <- mvrnorm(1, mu = mu, Sigma = lambda*sigma)
  return(distro)
}

# Log-likelihood function for logistic regression
logistic_ll <- function(beta, X, y) {
  z <- X %*% beta
  p <- 1 / (1 + exp(-z))
  loglike <- sum(log(p[y==1]))+sum(log((1-p)[y==0]))
  return(loglike)
}

# Log-prior of parameter that comes from proposal distro
library(mvtnorm)
log_prior <- function(distro, mu, sigma) {
  # Compute the density at 'distro' under the MVN used in proposal()
  prob <- dmvnorm(distro, mean = mu, sigma = sigma)
  log_prob <- log(prob)
  return(log_prob)
}

#Define log_prior_kde
log_prior_kde <- function(beta, C, den_funs) {
  prior_d = 0
  d <- 1 #iterative
  while (d <= dim(C)[2]) {
    if ( min(C[,d]) < beta[d] && beta[d] < max(C[,d]) ){
      density_at_point <- den_funs[[d]](beta[d])
      prior_d <- prior_d + log(density_at_point) #Assume independence
    }
    else {
      prior_d <- -10000
    }
    d <- d+1
  }
  return(prior_d)
}
```

Metropolis function:

```{r}
metropolis <- function(X, y, N = 20000, prior = TRUE, update = FALSE, KDE = FALSE, C = NA, den_funs = NA, M = ncol(X), a_target = 0.234, b1 = rep(0, M), batch = TRUE, batch_schedule = seq(50, N, by = 50)) {
  #0) Initial values
  if (batch == FALSE) {
    a_target <- a_target/2.26 
  }
  n_accept <- 0 #number of acceptances
  b <- matrix(0, nrow = N, ncol = M)
  b[1,] <- b1 #rep(0, M)
  
  #Keep history of proposals
  P_list <- rep(NA, N - 1)
  P_list[1] <- logistic_ll(b[1,], X, y)
  
  #Adaptation parameters
  batch_index <- 1
  Tb <- 0 #start of batch
  mu <- rep(0, M)
  Sigma <- diag(M)
  lambda <- (2.38**2)/M
  gam <- 1
  
  for (i in 2:N) {
    #1) Propose new position beta[i-1] -> beta[i]' from proposal distro
    if (update == FALSE) {
      b_prop <- proposal(mu, lambda, Sigma)
    } else {
      r <- sample(2:N, 1)
      b_prop <- C[r, ]
    }
    
    #2) Compute transition probability
    if (prior == TRUE) {
      if (KDE == FALSE) {
        P_list[i] <- logistic_ll(b_prop, X, y) + log_prior(b_prop, rep(0, M), diag(M))  #P(beta*)
      } else {
        P_list[i] <- logistic_ll(b_prop, X, y) + log_prior_kde(b_prop, C, den_funs)
      }
    } else {
      P_list[i] <- logistic_ll(b_prop, X, y)
    }
    
    #3) Acceptance ratio
    R <- exp(P_list[i] - P_list[i-1])
    alpha <- min(R, 1)
    
    #4) Generate a random number u_i from [0,1]. Accept or reject proposal
    u <- runif(1)  # u_i
    if (u <= alpha) {
        b[i,] <- b_prop  # Accept y as next state
        n_accept <- n_accept + 1
    } else {
        b[i,] <- b[i-1,]  # Stay at previous state of Markov chain
        P_list[i] <- P_list[i-1]
    }
    
    #5) Adaptive step
    if (batch == TRUE) {
      if (batch_index <= length(batch_schedule) && i == batch_schedule[batch_index]) {
        #Batch update
        Tb1 <- i #T_{b+1}
        b_batch <- b[(Tb + 1):Tb1, , drop = FALSE]
        n_batch <- nrow(b_batch)
        
        #Covar update
        cov_sum <- matrix(0, M, M)
        for (j in 1:n_batch) {
          diff <- matrix(b_batch[j,] - mu, ncol=1)
          cov_sum <- cov_sum + (diff%*%t(diff) - Sigma)
        }
        Sigma <- Sigma + cov_sum/n_batch + 1e-5*diag(M)
        
        #Mean update
        mean_diff <- colMeans(b_batch) - mu
        mu <- mu + mean_diff
        
        #Parameter update
        Tb <- Tb1
        batch_index <- batch_index + 1
        lambda <- exp( log(lambda) + (alpha-a_target)/n_batch )
      }
    } else { #Batch == FALSE
      gam <- 1/i
      Sigma <- Sigma + gam*( (b[i,]-mu)%*%t(b[i,]-mu) - Sigma )
      mu <- mu + gam*( b[i,]-mu )
      lambda <- exp( log(lambda) + gam*(alpha-a_target) )
    }
  }

  n <- as.integer(0.9 * N)
  C <- b[(N - n + 1):N,] #discard first 10% of samples
  
  cat("Acceptance rate:", n_accept/(N-1), "\n")
  return(C)
}
```

Prior Recursive function:

```{r}
prior_recursive <- function(k, X, y, N = 20000, M = ncol(X), a_target = 0.234, b1 = rep(0, M)) {
  #Partition the data
  #Number of rows and columns
  n <- nrow(X)
  
  # Get row group assignments
  part_indices <- cut(seq_len(n), breaks = k, labels = FALSE)
  
  # Split row indices instead of splitting X directly
  index_groups <- split(seq_len(n), part_indices)
  
  # Turn each group of indices into a matrix
  parts_X <- lapply(index_groups, function(idx) X[idx, , drop = FALSE])
  y <- matrix(y, ncol = 1)
  parts_y <- lapply(index_groups, function(idx) y[idx, , drop = FALSE])
  
  #Prior recursive implementation
  for (j in seq(1,k)) { 
    if (j == 1) {
      C <- metropolis(parts_X[[j]], parts_y[[j]], N = N, prior = TRUE, update = FALSE, KDE = FALSE, C = NA, den_funs = NA, a_target = a_target, b1 = b1, batch = TRUE)
    
    } else {
      
      #Create KDE functions and MAP (modes) vals
      den_funs <- list()
      modes <- list()
      d <- 1 #iterative
      while (d <= dim(C)[2]) {
        kde <- density(C[,d])
        den_funs[[d]] <- approxfun(kde$x, kde$y) #approx fun
        modes[[d]] <- kde$x[which.max(kde$y)]
        d <- d+1
      }
      modes <- unlist(modes)
      
      C <- metropolis(parts_X[[j]], parts_y[[j]], N = N, prior = TRUE, update = FALSE, KDE = TRUE, C = C, den_funs = den_funs, a_target = a_target, b1 = modes, batch = TRUE)
      
    }
  }
  return(C)
}
```

Test:

```{r}
C = prior_recursive(3, X, y)
```

$\beta$ distro estimate:

```{r}
c <- C[, 1]
library(ggplot2)
h <- 3.5 * sd(c) / (length(c)^(1/3))
nbins <- ceiling((max(c) - min(c)) / h)
ggplot() +
  geom_histogram(aes(x = c, y = after_stat(density)), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "l", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```

```{r}
library(coda)
mcmc_obj <- as.mcmc(C)
traceplot(mcmc_obj)
```

```{r}
autocorr.plot(mcmc_obj)
```

```{r}
effectiveSize(mcmc_obj)
```

References:

-   Hooten et al. *Making Recursive Bayesian Inference Accessible.*
