---
title: "Recursive Bayesian Methods"
author: "Alexander Berliner"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## (i) Simulating Data from a Logistic Regression Model with Synthetic Predictors
Logistic regression is a binary classifier. We want to find the probability that the observation $P(y = 1 \mid \mathbf{x})$ is a member of a class. 

Along with the training set $\mathbf{X}$, we have a vector $\boldsymbol{\beta}$ of weights for each feature, including the intercept. The resulting evidence for the class is given by:

$$
z = \mathbf{X} \boldsymbol{\beta}
$$

To convert $z$ into a probability, we pass it through the sigmoid (logistic) function:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

Reference: [https://web.stanford.edu/~jurafsky/slp3/5.pdf](https://web.stanford.edu/~jurafsky/slp3/5.pdf)
```{r}
n = 2000 #number of observations (set it higher for consistency)
p = 5 #number of predictors

X <- cbind(1, matrix(rnorm(n * p), n, p))  # Design matrix with intercept (bias). nxp
beta <- c(1, 2, 3, 4, 5, 6)  # True coefficients (bias, and predictors) px1

z <- X %*% beta  # Linear predictor. %*% = matrix mult. nx1 dim
sig <- 1/(1+exp(-z)) #sigmoid (inv-logit) function, find probability in [0,1], predicted y

y = rbinom(n=n, size=1, prob=sig) #size=num trials=1 -> bernoulli response var
```

## (ii) Sample from the posterior distribution of the regression coefficients using conventional Bayesian methods.
Define initial functions:
```{r}
#Proposal distro
library(MASS)
proposal <- function(i, M, beta, a = 0.01) {
  # Draw a single sample from multivariate normal
  distro <- mvrnorm(1, mu = beta[i-1,], Sigma = a*diag(M))
  return(distro)
}

# Log-likelihood function for logistic regression
logistic_ll <- function(beta, X, y) {
  z <- X %*% beta
  p <- 1 / (1 + exp(-z))
  loglike <- sum(log(p[y==1]))+sum(log((1-p)[y==0]))
  return(loglike)
}

# Uniform Log-prior of unknown parameter
log_prior_uniform <- function(beta) {
  prior = 0
  return(prior)
}
```

Metropolis implementation:
```{r}
#0) Initial values
n_accept <- 0 #number of acceptances
N <- 10000 #number of samples
M <- p+1 #dimensions
b <- matrix(0, nrow = N, ncol = M) #empty matrix to be filled with vals
b[1,] <- rep(0, M)  # first value of beta in the Markov chain is (0,0).

#Keep history of proposals
P_list <- rep(NA, N - 1)
P_list[1] = logistic_ll(b[1,], X, y) + log_prior_uniform(b[1])  #P(beta^{i-1})
for (i in 2:N) {
    #1) Propose new position beta[i-1] -> beta[i]' from proposal distro
    b_prop <- proposal(i, M, b)
    
    #2) Compute transition probability
    P_list[i] <- logistic_ll(b_prop, X, y) + log_prior_uniform(b_prop)  #P(beta*)
    while (is.nan(P_list[i])) {
      b_prop <- proposal(i, M, b)
      P_list[i] <- logistic_ll(b_prop, X, y) + log_prior_uniform(b_prop)  #P(beta*)
    }
    
    #3) Acceptance ratio
    alpha <- min(exp(P_list[i] - P_list[i-1]), 1)
    
    #3) Generate a random number u_i from [0,1]
    u <- runif(1)  # u_i
    
    #4)
    if (u <= alpha) {
        b[i,] <- b_prop  # Accept y as next state
        n_accept = n_accept + 1
    } else {
        b[i,] <- b[i-1,]  # Stay at previous state of Markov chain
        P_list[i] = P_list[i-1]
    }
}

n <- as.integer(0.9 * N)
C <- b[(N - n + 1):N,] #discard first 10% of samples

cat("Rate of acceptance:", n_accept/(N-1), "\n")
```
$\beta$ distro estimate:
```{r}
c = C[,6]
# Plot of density function
library(ggplot2)
nbins <- ceiling( (2*n)**(1/3) ) #Terrell-Scott rule for min num bins
ggplot() +
  geom_histogram(aes(x = c, y = ..density..), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "l", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```

```{r}
library(coda)
mcmc_obj <- as.mcmc(C)
traceplot(mcmc_obj)
```
```{r}
autocorr.plot(mcmc_obj)
```

## (iii) Implementing a recursive approach in which we consider batches of data in a sequence
First we partition the dataset
```{r}
#Number of partitions in dataset
k <- 2

#Number of rows
n <- nrow(X)

# Get row group assignments
part_indices <- cut(seq_len(n), breaks = k, labels = FALSE)

# Split row indices instead of splitting X directly
index_groups <- split(seq_len(n), part_indices)

# Turn each group of indices into a matrix
parts_X <- lapply(index_groups, function(idx) X[idx, , drop = FALSE])
y <- matrix(y, ncol = 1)
parts_y <- lapply(index_groups, function(idx) y[idx, , drop = FALSE])
```

**2.1) Prior-recursive Bayesian Inference**
```{r}
#0) Initial values
n_accept <- 0
a <- 0.01 #error in proposal distro
N <- 5000 #number of samples
M <- p+1 #dimensions
    
for (j in seq(1,k)) { 
  if (j == 1) {
    
    #Iteration j==1
    #====================
    #0) Initial values
    b <- matrix(0, nrow = N, ncol = M) #empty matrix to be filled with vals
    b[1,] <- rep(0, M)  # first value of beta in the Markov chain is (0,0).
    
    #Keep history of proposals
    P_list <- rep(NA, N - 1)
    P_list[1] = logistic_ll(b[1,], parts_X$`1`, parts_y$`1`)+log_prior_uniform(b[1])  #P(beta^{i-1})
    for (i in 2:N) {
        #1) Propose new position beta[i-1] -> beta[i]' from proposal distro
        b_prop <- proposal(i, M, b, a)
        
        #2) Compute transition probability
        P_list[i] <- logistic_ll(b_prop, parts_X$`1`, parts_y$`1`)+log_prior_uniform(b_prop)  #P(beta*)
        while (is.nan(P_list[i])) {
          b_prop <- proposal(i, M, b, a)
          P_list[i] <- logistic_ll(b_prop, parts_X$`1`, parts_y$`1`)+log_prior_uniform(b_prop)  #P(beta*)
        }
        
        #3) Acceptance ratio
        alpha <- min(exp(P_list[i] - P_list[i-1]), 1)
        
        #3) Generate a random number u_i from [0,1]
        u <- runif(1)  # u_i
        
        #4)
        if (u <= alpha) {
            b[i,] <- b_prop  # Accept y as next state
            n_accept = n_accept + 1
        } else {
            b[i,] <- b[i-1,]  # Stay at previous state of Markov chain
            P_list[i] = P_list[i-1]
        }
    }
    n <- as.integer(0.9 * N)
    C <- b[(N - n + 1):N,] #discard first 10% of samples
  
    } else {
      
    #Iteration j > 1
    #====================
    #Create KDE functions
    den_funs <- list()
    d = 1
    while (d <= dim(C)[2]) {
      kde <- density(C[,d])
      den_funs[[d]] <- approxfun(kde$x, kde$y) #approx fun
      d = d+1
    }
    
    #Define log_prior
    log_prior <- function(beta) {
      prior_d = 0
      d = 1
      while (d <= dim(C)[2]) {
        if ( min(C[,d]) < beta[d] && beta[d] < max(C[,d]) ){
          density_at_point = den_funs[[d]](beta[d])
          prior_d = prior_d + log(density_at_point) #Assume independence
        }
        else {
          prior_d = -10000
        }
        d = d+1
      }
      return(prior_d)
    }
    
    #0) Initial values
    b <- matrix(0, nrow = N, ncol = M) #empty matrix to be filled with vals
    b[1,] <- rep(0, M)  # first value of beta in the Markov chain is (0,0).
    
    #Keep history of proposals
    P_list <- rep(NA, N - 1)
    P_list[1] <- logistic_ll(b[1,], parts_X[[j]], parts_y[[j]]) + log_prior(b[1,])  #P(beta^{i-1})
    for (i in 2:N) {
        #1) Propose new position beta[i-1] -> beta[i]' from proposal distro
        b_prop <- proposal(i, M, b, a)
        
        #2) Compute transition probability
        P_list[i] <- logistic_ll(b_prop, parts_X[[j]], parts_y[[j]]) + log_prior(b_prop)  #P(beta*)
        while (is.nan(P_list[i])) {
          b_prop <- proposal(i, M, b, a)
          P_list[i] <- logistic_ll(b_prop, parts_X[[j]], parts_y[[j]]) + log_prior(b_prop)  #P(beta*)
        }
        
        #3) Acceptance ratio
        alpha <- min(exp(P_list[i] - P_list[i-1]), 1)
        
        #3) Generate a random number u_i from [0,1]
        u <- runif(1)  # u_i
        
        #4)
        if (u <= alpha) {
            b[i,] <- b_prop  # Accept y as next state
            n_accept = n_accept + 1
        } else {
            b[i,] <- b[i-1,]  # Stay at previous state of Markov chain
            P_list[i] = P_list[i-1]
        }
    }
    n <- as.integer(0.9 * N)
    C <- b[(N - n + 1):N,] #discard first 10% of samples
  }
}

cat("Rate of acceptance:", n_accept/((N-k)*k), "\n")
```
$\beta$ distro estimate:
```{r}
c = C[,6]
# Plot of density function
library(ggplot2)
nbins <- ceiling( (2*n)**(1/3) ) #Terrell-Scott rule for min num bins
ggplot() +
  geom_histogram(aes(x = c, y = ..density..), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "l", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```
```{r}
mcmc_obj <- as.mcmc(C)
traceplot(mcmc_obj)
```
```{r}
autocorr.plot(mcmc_obj)
```

**2.2)/2.3) Prior-proposal recursive Bayesian Inference**
```{r}
#0) Initial values
n_accept <- 0
N_total <- 0
a <- 0.001 #error in proposal distro
M <- p+1 #dimensions
    
for (j in seq(1,k)) { 
  if (j == 1) {
    
    #Iteration j == 1
    #====================
    #0) Initial values
    N <- 10000 #number of samples
    N_total = N_total + N - 1
    b <- matrix(0, nrow = N, ncol = M) #empty matrix to be filled with vals
    b[1,] <- rep(0, M)  # first value of beta in the Markov chain is (0,0).
    
    #Keep history of proposals
    P_list <- rep(NA, N - 1)
    P_list[1] <- logistic_ll(b[1,], parts_X$`1`, parts_y$`1`) #P(beta^{i-1})
    for (i in 2:N) {
        #1) Propose new position beta[i-1] -> beta[i]' from proposal distro
        b_prop <- proposal(i, M, b, a)
        
        #2) Compute transition probability
        P_list[i] <- logistic_ll(b_prop, parts_X$`1`, parts_y$`1`) #P(beta*)
        while (is.nan(P_list[i])) {
          b_prop <- proposal(i, M, b, a)
          P_list[i] <- logistic_ll(b_prop, parts_X$`1`, parts_y$`1`) #P(beta*)
        }
        
        #3) Acceptance ratio
        alpha <- min(exp(P_list[i] - P_list[i-1]), 1)
        
        #3) Generate a random number u_i from [0,1]
        u <- runif(1)  # u_i
        
        #4)
        if (u <= alpha) {
            b[i,] <- b_prop  # Accept y as next state
            n_accept = n_accept + 1
        } else {
            b[i,] <- b[i-1,]  # Stay at previous state of Markov chain
            P_list[i] = P_list[i-1]
        }
    }
    n <- as.integer(0.9 * N)
    C <- b[(N - n + 1):N,] #discard first 10% of samples
    print(dim(C))
    
  } else {
    
    #Iteration j > 1
    #====================
    #0) Initial values
    N <- dim(C)[1] #number of samples
    N_total = N_total + N - 1
    b <- matrix(0, nrow = N, ncol = M) #empty matrix to be filled with vals
    b[1,] <- C[1,]  # first value of beta in the Markov chain is (0,0).
    
    #Keep history of proposals
    P_list <- rep(NA, N - 1)
    P_list[1] <- logistic_ll(b[1,], parts_X[[j]], parts_y[[j]]) #P(beta^{i-1})
    for (i in 2:N) {
        #1) Propose new position beta[i-1] -> beta[i]' from proposal distro
        b_prop <- C[i,]
        
        #2) Compute transition probability
        P_list[i] <- logistic_ll(b_prop, parts_X[[j]], parts_y[[j]]) #P(beta*)
        
        #3) Acceptance ratio
        alpha <- min(exp(P_list[i] - P_list[i-1]), 1)
        
        #3) Generate a random number u_i from [0,1]
        u <- runif(1)  # u_i
        
        #4)
        if (u <= alpha) {
            b[i,] <- b_prop  # Accept y as next state
            n_accept = n_accept + 1
        } else {
            b[i,] <- b[i-1,]  # Stay at previous state of Markov chain
            P_list[i] = P_list[i-1]
        }
    }
    n <- as.integer(0.9 * N)
    C <- b[(N - n + 1):N,] #discard first 10% of samples
    print(dim(C))
  }
}

cat("Rate of acceptance:", n_accept/N_total, "\n")
```
$\beta$ distro estimate:
```{r}
c = C[,2]
# Plot of density function
library(ggplot2)
nbins <- ceiling( (2*n)**(1/3) ) #Terrell-Scott rule for min num bins
ggplot() +
  geom_histogram(aes(x = c, y = ..density..), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "l", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```
```{r}
mcmc_obj <- as.mcmc(C)
traceplot(mcmc_obj)
```
```{r}
autocorr.plot(mcmc_obj)
```
