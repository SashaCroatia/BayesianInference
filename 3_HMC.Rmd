---
title: "MCMC"
author: "Alexander Berliner"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

### (i) Simulating Data from a Logistic Regression Model with Synthetic Predictors

Logistic regression is a binary classifier. We want to find the probability that the observation $P(y = 1 \mid \mathbf{x})$ is a member of a class.

Along with the training set $\mathbf{X}$, an $n\text{x}p$ matrix of $n$ observations and $M$ predictors, including one column of 1s for the intercept, we also have a $M\text{x}1$ vector $\boldsymbol{\beta}$ of weights for each feature, including the intercept. The resulting evidence for the class is given by:

$$
z = \mathbf{X} \boldsymbol{\beta}
$$

To convert $z$ into a probability, we pass it through the sigmoid (logistic) function:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

**References**:

1.  <https://web.stanford.edu/~jurafsky/slp3/5.pdf>

**Setup:**

```{r}
#To be set by user:
n = 2000 #number of observations (set it higher for consistency)
beta <- c(1, 2, 3)#, 4, 5, 6)  # True coefficients (bias, and predictors) px1

#Synthetic predictors
p = length(beta)-1 #number of predictors
M = p+1 #number of predictors plus intercept
library(MASS)
X <- cbind(1, matrix(mvrnorm(n, mu = rep(0, p), Sigma = 1*diag(p)), n, p))  # Design matrix with intercept (bias). nxp

#Response
z <- X %*% beta  # Linear predictor. %*% = matrix mult. nx1 dim
sig <- 1/(1+exp(-z)) #sigmoid (inv-logit) function, find probability in [0,1], predicted y
y = rbinom(n=n, size=1, prob=sig) #size=num trials=1 -> bernoulli response var
```

### (ii) HMC sampling

Notes on HMC sampling are in 3_HMC_notes.ipynb file.

**HMC implementation:**

```{r}
hmc <- function(X, y, niter = 5000, nburnin = 1000, nchains = 1, inits = NULL) {
  #1) Define model
  #---------------
  library(nimble)
  log_reg_code <- nimbleCode({
    #Piors
    for (j in 1:M) {
      beta[j] ~ dnorm(mean = 0, sd = 100)
    }
    
    #Likelihood
    for (i in 1:n) {
      logit(p[i]) <- inprod(X[i, 1:M], beta[1:M])
      y[i] ~ dbern(p[i]) 
    }
  })
  
  #2) Prepare data, constants, init values
  #---------------
  hmc_consts <- list(n = nrow(X), M = ncol(X))
  
  hmc_data <- list(y = y, X = X)
  
  if(is.null(inits)){
    hmc_inits <- list(beta = rep(0, ncol(X)))
  } else {
    hmc_inits <- inits
  }
  
  #3) Create model
  #---------------
  model <- nimbleModel(
    code = log_reg_code,
    constants = hmc_consts,
    data = hmc_data,
    inits = hmc_inits,
    calculate = FALSE,
    buildDerivs = TRUE
  )
  
  #4) Compile model and mcmc to C++ for speed
  #---------------
  compiled_model <- compileNimble(model)
  
  #5) Configure model
  #---------------
  library(nimbleHMC)
  model_config <- configureHMC(model)
  model_config$removeSamplers() #remove all
  model_config$addSampler(target = "beta")
  
  #6) Build HMC algo
  #---------------
  HMC <- buildHMC(model = compiled_model, hmcConf = model_config)
  
  #7) Compile HMC algo
  #---------------
  compiled_HMC <- compileNimble(HMC, project = model)
  
  #8) Run HMC sampler
  #---------------
  samples <- runMCMC(
    compiled_HMC,
    niter = niter,
    nburnin = nburnin,
    nchains = nchains,
    summary = TRUE
  )
  
  return(samples)
}
```

```{r}
samples = hmc(X,y)
```

### (iii) Diagnostics

**Summary statistics** of the samples:

```{r}
samples$summary
```

**Visualization** of the posterior sample distribution of one of the $\beta_i$:

```{r}
c = samples$samples[,1]
# Plot of density function
library(ggplot2)
nbins <- ceiling( (2*n)**(1/3) ) #Terrell-Scott rule for min num bins
ggplot() +
  geom_histogram(aes(x = c, y = ..density..), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "beta", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```

**Traceplots** of each $\beta_i$. Suppose our MCMC samples has $T$ sample draws $x = x_1,\ldots, x_T$ where each $x_t$ is a $M\text{x}1$ vector. Illustrates the sample draws $x$ as for each increment of time $t$ to determine if there are any apparent serial correlations between samples or between each target parameter $\beta_i$. A lot of serial correlation between successive draws indicate few independent observations in the samples.

```{r}
library(coda)
mcmc_obj <- as.mcmc(samples$samples)
traceplot(mcmc_obj)
```

**Autocorrelation plots** of each $\beta_i$. These would correspond with the conclusions from the traceplots. Low serial correlation corresponds to high autocorrelation at short lags, where a lag is the number of timesteps one sample draw is from another.

```{r}
autocorr.plot(mcmc_obj)
```

**Effective sample size** of each $\beta_i$. A measure of the number of independent samples from the autocorrelated samples from the mcmc output. In other words, it gives us an assessment of the amount of information contained in the mcmc output, rather than looking at the total number of iterations. A high ESS indicates efficient sampling.

```{r}
effectiveSize(mcmc_obj)
```

**WAIC** values (how well will model predict data it wasn't trained on; equivalent to cross-validation loss; lower values mean better performance).
