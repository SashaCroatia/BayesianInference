---
title: "MCMC"
author: "Alexander Berliner"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---

## (i) Simulating Data from a Logistic Regression Model with Synthetic Predictors

Logistic regression is a binary classifier. We want to find the probability that the observation $P(y = 1 \mid \mathbf{x})$ is a member of a class.

Along with the training set $\mathbf{X}$, we have a vector $\boldsymbol{\beta}$ of weights for each feature, including the intercept. The resulting evidence for the class is given by:

$$
z = \mathbf{X} \boldsymbol{\beta}
$$

To convert $z$ into a probability, we pass it through the sigmoid (logistic) function:

$$
\sigma(z) = \frac{1}{1 + e^{-z}}
$$

**References**:

1.  <https://web.stanford.edu/~jurafsky/slp3/5.pdf>

**Setup:**

```{r}
#To be set by user:
n = 2000 #number of observations (set it higher for consistency)
beta <- c(1, 2, 3)#, 4, 5, 6)  # True coefficients (bias, and predictors) px1

#Synthetic predictors
p = length(beta)-1 #number of predictors
M = p+1 #number of predictors plus intercept
library(MASS)
X <- cbind(1, matrix(mvrnorm(n, mu = rep(0, p), Sigma = 1*diag(p)), n, p))  # Design matrix with intercept (bias). nxp

#Response
z <- X %*% beta  # Linear predictor. %*% = matrix mult. nx1 dim
sig <- 1/(1+exp(-z)) #sigmoid (inv-logit) function, find probability in [0,1], predicted y
y = rbinom(n=n, size=1, prob=sig) #size=num trials=1 -> bernoulli response var
```

## (ii) HMC sampling

Notes on HMC sampling are in 3_HMC_notes.ipynb file.

**Define initial functions:**

```{r}
#Proposal distro
proposal <- function(mu, lambda, sigma) {
  # Draw a single sample from multivariate normal
  distro <- mvrnorm(1, mu = mu, Sigma = lambda*sigma)
  return(distro)
}

# Log-likelihood function for logistic regression
logistic_ll <- function(beta, X, y) {
  z <- X %*% beta
  p <- 1 / (1 + exp(-z))
  loglike <- sum(log(p[y==1]))+sum(log((1-p)[y==0]))
  return(loglike)
}

# Log-prior of parameter that comes from proposal distro
library(mvtnorm)
log_prior <- function(distro, mu, sigma) {
  # Compute the density at 'distro' under the MVN used in proposal()
  prob <- dmvnorm(distro, mean = mu, sigma = sigma)
  log_prob = log(prob)
  return(log_prob)
}
```

**HMC implementation:**

```{r}
hmc <- function(X, y, niter = 10000, nburnin = 2500, nchains = 1, inits = NULL) {
  #1) Define model
  #---------------
  library(nimble)
  log_reg_code <- nimbleCode({
    #Piors
    for (j in 1:M) {
      beta[j] ~ dnorm(mean = 0, sd = 100)
    }
    
    #Likelihood
    for (i in 1:n) {
      logit(p[i]) <- inprod(X[i, 1:M], beta[1:M])
      y[i] ~ dbern(p[i]) 
    }
  })
  
  #2) Prepare data, constants, init values
  #---------------
  hmc_consts <- list(n = nrow(X), M = ncol(X))
  
  hmc_data <- list(y = y, X = X)
  
  if(is.null(inits)){
    hmc_inits <- list(beta = rep(0, ncol(X)))
  } else {
    hmc_inits <- inits
  }
  
  #3) Create model
  #---------------
  model <- nimbleModel(
    code = log_reg_code,
    constants = hmc_consts,
    data = hmc_data,
    inits = hmc_inits,
    calculate = FALSE,
    buildDerivs = TRUE
  )
  
  #4) Configure model; non-HMC
  #---------------
  # mcmc_config <- configureMCMC(model)
  # mcmc_config$removeSamplers('beta') #remove defualt whatever it is
  
  # #Configure for hmc
  # mcmc_config$addSampler(target = 'beta')
  
  #5) Build MCMC from configuration; non-HMC
  #---------------
  # mcmc <- buildMCMC(mcmc_config)
  
  #6) Compile model and mcmc to C++ for speed
  #---------------
  compiled_model <- compileNimble(model)
  
  #7) Build HMC algo
  #---------------
  library(nimbleHMC)
  HMC <- buildHMC(model)
  
  #8) Compile HMC algo
  #---------------
  compiled_HMC <- compileNimble(HMC, project = model)
  
  #9) Run HMC sampler
  #---------------
  samples <- runMCMC(
    compiled_HMC,
    niter = niter,
    nburnin = nburnin,
    nchains = nchains,
    summary = TRUE
  )
  
  return(samples)
}
```

```{r}
samples = hmc(X,y)
```

$\beta$ distribution estimate:

```{r}
c = samples$samples[,1]
# Plot of density function
library(ggplot2)
nbins <- ceiling( (2*n)**(1/3) ) #Terrell-Scott rule for min num bins
ggplot() +
  geom_histogram(aes(x = c, y = ..density..), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "l", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```

Traceplots

```{r}
library(coda)
mcmc_obj <- as.mcmc(samples$samples)
traceplot(mcmc_obj)
```

Autocorrelation plots

```{r}
autocorr.plot(mcmc_obj)
```
