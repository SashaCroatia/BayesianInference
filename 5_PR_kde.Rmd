---
title: "Prior Recursive"
author: "Alexander Berliner"
date: "`r Sys.Date()`"
output:
  html_document:
    df_print: paged
  pdf_document: default
---a
---

### (i) Simulating Data from a Logistic Regression Model with Synthetic Predictors

Logistic regression is a binary classifier. We want to find the probability that the observation $P(y = 1 \mid \mathbf{x})$ is a member of a class.

Along with the training set $\mathbf{X}$, an $n\text{x}p$ matrix of $n$ observations and $M$ predictors, including one column of 1s for the intercept, we also have a $M\text{x}1$ vector $\boldsymbol{\beta}$ of weights for each feature, including the intercept. The resulting evidence for the class is given by:

$$ z = \mathbf{X} \boldsymbol{\beta} $$

To convert $z$ into a probability, we pass it through the sigmoid (logistic) function:

$$ \sigma(z) = \frac{1}{1 + e^{-z}} $$

**References**:

1.  <https://web.stanford.edu/~jurafsky/slp3/5.pdf>

**Setup:**

```{r}
#To be set by user:
n = 2000 #number of observations (set it higher for consistency)
beta <- c(1, 2, 3)#, 4, 5, 6)  # True coefficients (bias, and predictors) px1

#Synthetic predictors
p = length(beta)-1 #number of predictors
M = p+1 #number of predictors plus intercept
library(MASS)
X <- cbind(1, matrix(mvrnorm(n, mu = rep(0, p), Sigma = 1*diag(p)), n, p))  #Design matrix with intercept (bias). nxp

#Response
z <- X %*% beta  # Linear predictor. %*% = matrix mult. nx1 dim
sig <- 1/(1+exp(-z)) #sigmoid (inv-logit) function, find probability in [0,1], predicted y
y = rbinom(n=n, size=1, prob=sig) #size=num trials=1 -> bernoulli response var
```

### (ii) Implementing a recursive approach in which we consider batches of data in a sequence

**2.1) Prior-recursive Bayesian Inference [1]**

**User-defined distributions [3]**

```{r}
kde_wrapper <- function(x, samples) {
  kde <- density(samples)
  kde_func <- approxfun(kde$x, kde$y)
  kde_func(x)
}

library(nimble)
dkde <- nimbleFunction(
  setup = function() {
    kdeR <- nimbleRcall(
      function(x = double(0), samples = double(1)) {},
      Rfun = 'kde_wrapper',
      returnType = double(0)
    )
  },
  run = function(x = double(0), samples = double(1), log = integer(0, default = 0)) {
    returnType(double(0))
    
    dens <- kdeR(x, samples)
    if (dens <= 0) dens <- 1e-12
    if (log) return(log(dens))
    else return(dens)
  }
)

rkde <- nimbleFunction(
  run = function(n = integer(0), samples = double(1)) {
    returnType(double(0))

    len <- length(samples)
    idx <- as.integer(1 + floor(runif(1, 0, len)))  # uniform index from 1 to len
    out <- samples[idx]

    return(out)
  }
)
```

**HMC implementation [2]**

```{r}
hmc <- function(X, y, niter = 5000, nburnin = 1000, nchains = 1, inits = NULL, KDE = FALSE, C = NA) {
  n = nrow(X)
  M = ncol(X)
  
  #1) Define model
  #---------------
  library(nimble)
  if (KDE == FALSE) {
    log_reg_code <- nimbleCode({
      #Piors
      for (j in 1:M) {
        beta[j] ~ dnorm(mean = 0, sd = 100)
      }
      
      #Likelihood
      for (i in 1:n) {
        logit(p[i]) <- inprod(X[i, 1:M], beta[1:M])
        y[i] ~ dbern(p[i]) 
      }
    })
  } else {
    log_reg_code <- nimbleCode({
      #Priors
      for (j in 1:M) {
        beta[j] ~ dkde(C[1:n,j])
      }
      
      #Likelihood
      for (i in 1:n) {
        logit(p[i]) <- inprod(X[i, 1:M], beta[1:M])
        y[i] ~ dbern(p[i]) 
      }
    })
  }
  
  #2) Prepare data, constants, init values
  #---------------
  hmc_consts <- list(n = n, M = M)
  hmc_data <- list(y = y, X = X, C = C)
  
  if(is.null(inits)){
    hmc_inits <- list(beta = rep(0, M))
  } else {
    hmc_inits <- list(beta = inits)
  }
  
  #3) Create model
  #---------------
  model <- nimbleModel(
    code = log_reg_code,
    constants = hmc_consts,
    data = hmc_data,
    inits = hmc_inits,
    calculate = FALSE,
    buildDerivs = TRUE
  )
  
  #4) Compile model and mcmc to C++ for speed
  #---------------
  compiled_model <- compileNimble(model)
  
  #5) Configure model
  #---------------
  library(nimbleHMC)
  model_config <- configureHMC(model)
  model_config$removeSamplers() #remove all
  model_config$addSampler(target = "beta")
  
  #6) Build HMC algo
  #---------------
  HMC <- buildHMC(model = compiled_model, hmcConf = model_config)
  
  #7) Compile HMC algo
  #---------------
  compiled_HMC <- compileNimble(HMC, project = model)
  
  #8) Run HMC sampler
  #---------------
  samples <- runMCMC(
    compiled_HMC,
    niter = niter,
    nburnin = nburnin,
    nchains = nchains,
    summary = TRUE
  )
  
  return(samples)
}
```

**Prior Recursive function:**

```{r}
prior_recursive <- function(k, X, y, niter = 5000, nburnin = 1000) {
  #Partition the data
  #Number of rows
  n <- nrow(X)
  
  # Get row group assignments
  part_indices <- cut(seq_len(n), breaks = k, labels = FALSE)
  
  # Split row indices instead of splitting X directly
  index_groups <- split(seq_len(n), part_indices)
  
  # Turn each group of indices into a matrix
  parts_X <- lapply(index_groups, function(idx) X[idx, , drop = FALSE])
  y <- matrix(y, ncol = 1)
  parts_y <- lapply(index_groups, function(idx) y[idx, , drop = FALSE])
  
  #Prior recursive implementation
  for (j in seq(1,k)) { 
    if (j == 1) {
      
      C = hmc(parts_X[[j]], unlist(list(parts_y[[j]])), niter = niter, nburnin = nburnin, nchains = 1, inits = NULL, KDE = FALSE, C = NA)$samples
    
    } else {
      
      #Create KDE functions and MAP (modes) vals
      den_funs <- list()
      modes <- list()
      d = 1 #iterative
      while (d <= dim(C)[2]) {
        kde <- density(C[,d])
        den_funs[[d]] <- approxfun(kde$x, kde$y) #approx fun
        modes[[d]] <- kde$x[which.max(kde$y)]
        d = d+1
      }
      modes = unlist(modes)

      C = hmc(parts_X[[j]], unlist(list(parts_y[[j]])), niter = niter, nburnin = nburnin, nchains = 1, inits = modes, KDE = TRUE, C = C)$samples

    }
  }
  return(C)
}
```

```{r}
samples = prior_recursive(2,X,y)
```

**References:**

1.  Hooten et al. *Making Recursive Bayesian Inference Accessible.*
2.  <https://r-nimble.org/html_manual/cha-mcmc.html>
3.  <https://r-nimble.org/html_manual/cha-user-defined.html>

### (iii) Diagnostics

**Visualization** of the posterior sample distribution of one of the $\beta_i$:

```{r}
c = samples[,1]
# Plot of density function
library(ggplot2)
nbins <- ceiling( (2*n)**(1/3) ) #Terrell-Scott rule for min num bins
ggplot() +
  geom_histogram(aes(x = c, y = ..density..), bins = nbins, fill = "orange", color = "black", alpha = 0.6) +
  geom_density(aes(x = c), color = "blue", linewidth = 1) +
  labs(x = "l", y = "Density", title = "MCMC Approximation of the Posterior") +
  xlim(min(c), max(c)) +
  theme_minimal()
```

**Traceplots** of each $\beta_i$.

```{r}
library(coda)
mcmc_obj <- as.mcmc(samples)
traceplot(mcmc_obj)
```

**Autocorrelation plots** of each $\beta_i$.

```{r}
autocorr.plot(mcmc_obj)
```

**Effective sample size** of each $\beta_i$.

```{r}
effectiveSize(mcmc_obj)
```
